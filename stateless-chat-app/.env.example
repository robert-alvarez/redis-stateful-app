# ============================================================================
# OpenAI Configuration (for ChatGPT provider)
# ============================================================================
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-5-mini

# Alternative models:
# OPENAI_MODEL=gpt-5  # Better responses, more expensive
# OPENAI_MODEL=gpt-4o  # Fast and capable

# ============================================================================
# vLLM Configuration (for self-hosted inference on AWS)
# ============================================================================
# vLLM provides an OpenAI-compatible API endpoint for self-hosted model inference
# Supports both Chat Completions AND Responses APIs
VLLM_BASE_URL=http://your-vllm-server:8000/v1
VLLM_API_KEY=EMPTY  # vLLM doesn't require a real API key (use "EMPTY")
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct  # Change to your hosted model

# AWS Deployment Examples:
# VLLM_BASE_URL=http://ec2-12-34-56-78.compute-1.amazonaws.com:8000/v1  # EC2 public IP
# VLLM_BASE_URL=http://my-vllm-alb-123456.us-east-1.elb.amazonaws.com/v1  # ALB endpoint
# VLLM_BASE_URL=https://vllm.yourdomain.com/v1  # Custom domain with HTTPS

# How to deploy vLLM on AWS:
# 1. Launch GPU instance (g5.xlarge, g5.2xlarge, p3.2xlarge, etc.)
# 2. Install vLLM: pip install vllm
# 3. Start vLLM server with OpenAI-compatible API:
#    vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --host 0.0.0.0 --port 8000
# 4. Configure security groups to allow inbound traffic on port 8000
# 5. Update VLLM_BASE_URL with your server's public IP or domain
# 6. vLLM supports the Responses API for optimal performance!

# ============================================================================
# Redis Configuration (for conversation memory)
# ============================================================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=  # Leave empty if no password required

# Session Configuration
SESSION_TTL_SECONDS=3600  # Session expiration time (default: 1 hour)

# ============================================================================
# Quick Start Guide
# ============================================================================
# 1. Copy this file to .env: cp .env.example .env
# 2. Add your OpenAI API key to OPENAI_API_KEY
# 3. (Optional) Set up vLLM for local inference
# 4. Start Redis: redis-server
# 5. Start backend: python backend/main.py
# 6. Open frontend: open frontend/index.html
