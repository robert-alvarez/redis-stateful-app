# ğŸ¤– Redis Memory Magic - Chat API Playground

A demo application showcasing **stateless vs stateful** LLM applications with **Redis-backed conversation memory** and support for both **cloud (ChatGPT)** and **self-hosted (vLLM on AWS)** inference.

## âœ¨ Features

### Two Powerful Dimensions

**ğŸ’­ Memory Toggle**
- **Stateless:** No memory - watch it forget!
- **Stateful:** Redis-backed memory - remembers everything!

**ğŸ¢ Provider Toggle**
- **ChatGPT:** OpenAI's cloud API (fast, powerful)
- **vLLM:** Self-hosted inference on AWS (private, customizable, cost-effective)

### Under the Hood

- ğŸš€ **Responses API** - Latest OpenAI API for optimal performance
- ğŸ“¦ **Redis** - Fast in-memory conversation storage
- ğŸ¯ **FastAPI** - Modern Python backend
- ğŸ¨ **Clean UI** - Simple, responsive interface
- ğŸ³ **Docker Ready** - One-command deployment

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)

**One command to rule them all:**

```bash
./start.sh          # Mac/Linux
start.bat           # Windows
```

Or manually:

```bash
docker-compose up -d
```

**That's it!** Open http://localhost:3000

**Note:** For vLLM provider, you'll need a vLLM server running on AWS (see configuration below).

ğŸ‘‰ **Full Docker guide:** [DOCKER_README.md](DOCKER_README.md)

### Option 2: Local Development

**Requirements:**
- Python 3.11+
- Redis
- (Optional) OpenAI API key

**Backend:**
```bash
cd backend
pip install -r requirements.txt
cp ../.env.example .env
# Edit .env with your OpenAI API key
python main.py
```

**Frontend:**
```bash
cd frontend
# Open index.html in browser
# Or use: python -m http.server 8000
```

**Redis:**
```bash
redis-server
```

## ğŸ® Usage

1. **Open** http://localhost:3000
2. **Toggle memory** ON/OFF to see the difference
3. **Switch providers** between ChatGPT and vLLM (AWS)
4. **Test it:**
   - Say "My name is [Your Name]"
   - Ask "What's my name?"
   - Toggle memory to see the difference!

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚
â”‚  (Frontend) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Redis  â”‚
â”‚  (Backend)  â”‚      â”‚ Memory  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
   â”‚ LLM   â”‚
   â”‚       â”‚
   â”‚ChatGPTâ”‚  or  â”‚  vLLM   â”‚
   â”‚ (API) â”‚      â”‚ (AWS)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Key Concepts Demonstrated

### Stateless vs Stateful
- **Problem:** Stateless LLMs forget everything
- **Solution:** Redis maintains conversation history
- **Result:** Seamless memory management

### API Comparison
- **Responses API:** Better performance, lower costs
- **Benefits:** 3% accuracy gain, 40-80% cache improvement

### Provider Flexibility
- **Cloud (ChatGPT):** Fast, no setup, pay-per-use
- **Self-Hosted (vLLM):** Private, customizable, cost-effective at scale

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ backend/              # FastAPI application
â”‚   â”œâ”€â”€ main.py          # API endpoints
â”‚   â”œâ”€â”€ llm_service.py   # ChatGPT (stateless)
â”‚   â”œâ”€â”€ stateful_llm_service.py  # ChatGPT (stateful)
â”‚   â”œâ”€â”€ vllm_service.py  # vLLM (stateless)
â”‚   â”œâ”€â”€ stateful_vllm_service.py # vLLM (stateful)
â”‚   â”œâ”€â”€ memory_service.py # Redis integration
â”‚   â””â”€â”€ models.py        # Pydantic models
â”œâ”€â”€ frontend/            # Static web UI
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ style.css
â”œâ”€â”€ docker-compose.yml   # Full stack orchestration
â””â”€â”€ .env.example         # Configuration template
```

## ğŸ”§ Configuration

Edit `.env`:

```bash
# OpenAI (for ChatGPT)
OPENAI_API_KEY=your_key_here
OPENAI_MODEL=gpt-5-mini

# vLLM (for self-hosted inference on AWS)
VLLM_BASE_URL=http://your-vllm-server:8000/v1
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
```

## ğŸ³ Docker Deployment

See [DOCKER_README.md](DOCKER_README.md) for comprehensive Docker deployment guide including:
- Full stack setup with one command
- Production deployment tips
- Troubleshooting guide
- Distributed deployment strategies

## ğŸ› ï¸ Technology Stack

- **Backend:** FastAPI, Python 3.11
- **Frontend:** Vanilla JavaScript, CSS
- **Database:** Redis
- **AI:** OpenAI API, vLLM
- **Deployment:** Docker, Docker Compose, AWS

## ğŸ“š Learn More

- [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses)
- [Redis Documentation](https://redis.io/documentation)
- [vLLM Documentation](https://docs.vllm.ai/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

## ğŸ¤ Contributing

This is a demo project for educational purposes. Feel free to:
- Fork and experiment
- Suggest improvements
- Use as a template for your projects

## ğŸ“„ License

MIT License - feel free to use this for learning and building!

---

**Built with â¤ï¸ to demonstrate Redis-backed memory in LLM applications**

ğŸš€ **Get started now:** Run `./start.sh` and open http://localhost:3000
